{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 1\n",
    "# (a)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import time\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "import math\n",
    "\n",
    "def create_dataset(n=1250, nf=2, nr=0, ni=2, random_state=125):\n",
    "    '''\n",
    "    generate a new dataset with \n",
    "    n: total number of samples\n",
    "    nf: number of features\n",
    "    nr: number of redundant features (these are linear combinatins of informative features)\n",
    "    ni: number of informative features (ni + nr = nf must hold)\n",
    "    random_state: set for reproducibility\n",
    "    '''\n",
    "    X, y = make_classification( n_samples=n,\n",
    "                                n_features=nf,\n",
    "                                n_redundant=nr,\n",
    "                                n_informative=ni,\n",
    "                                random_state=random_state,\n",
    "                                n_clusters_per_class=2)\n",
    "    rng = np.random.RandomState(2)\n",
    "    X += 3 * rng.uniform(size = X.shape)\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (b\n",
    "def plotter(classifier, X, X_test, y_test, title, ax=None):\n",
    "    # plot decision boundary for given classifier\n",
    "    plot_step = 0.02\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:,0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:,1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step), \n",
    "                            np.arange(y_min, y_max, plot_step)) \n",
    "    Z = classifier.predict(np.c_[xx.ravel(),yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    if ax:\n",
    "        ax.contourf(xx, yy, Z, cmap = plt.cm.Paired)\n",
    "        ax.scatter(X_test[:, 0], X_test[:, 1], c = y_test)\n",
    "        ax.set_title(title)\n",
    "    else:\n",
    "        plt.contourf(xx, yy, Z, cmap = plt.cm.Paired)\n",
    "        plt.scatter(X_test[:, 0], X_test[:, 1], c = y_test)\n",
    "        plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q1_a(X, X_train, X_test, y_train, y_test):\n",
    "    # split the dataset, train=0.8; test = 0.2\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=45)\n",
    "    ## 1- Decision Tree Classifier: DT \n",
    "    DT_clf = DecisionTreeClassifier()\n",
    "    DT_clf.fit(X_train,y_train)\n",
    "    plt.figure(figsize=(12,12))\n",
    "    plt.subplot(3,2,1)\n",
    "    plotter(DT_clf, X, X_test, y_test, \"Decision Tree Classifier\")\n",
    "    # 2- Model: Random Forest Classifier: RF\n",
    "    RF_clf = RandomForestClassifier()\n",
    "    RF_clf.fit(X_train,y_train)\n",
    "    plt.subplot(3,2,2), plotter(RF_clf, X, X_test, y_test, title=\"Random Forest Classifier\")\n",
    "    # 3- AdaBoost Classifier: AB\n",
    "    AB_clf = AdaBoostClassifier()\n",
    "    AB_clf.fit(X_train,y_train)\n",
    "    plt.subplot(3,2,3), plotter(AB_clf, X, X_test, y_test, title = \"AdaBoost Classifier\")\n",
    "    # 4- Logistic Regression Classifier: LR\n",
    "    LR_clf = LogisticRegression()\n",
    "    LR_clf.fit(X_train,y_train)\n",
    "    plt.subplot(3,2,4), plotter(LR_clf, X, X_test, y_test, title=\"Logistic Regression Classifier\")\n",
    "    # 5- Multilayer Perceptron(Neural Network) Classifier: MLP\n",
    "    MLP_clf = MLPClassifier()\n",
    "    MLP_clf.fit(X_train,y_train)\n",
    "    plt.subplot(3,2,5), plotter(MLP_clf, X, X_test, y_test, title=\"Multilayer Perceptron(Neural Network) Classifier\")\n",
    "    # 6- Support Vector Machine Classifier: SVM\n",
    "    SVM_clf = SVC()\n",
    "    SVM_clf.fit(X_train,y_train)\n",
    "    plt.subplot(3,2,6), plotter(SVM_clf, X, X_test, y_test, title=\"Support Vector Machine Classifier\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_accuracy(clf, X_test, y_test):\n",
    "    accuracy_list = []\n",
    "    for i in range(10):\n",
    "        y_predict = clf.predict(X_test)\n",
    "        acc= accuracy_score(y_test, y_predict)\n",
    "        accuracy_list.append(acc)\n",
    "    mean_acc = np.mean(accuracy_list)\n",
    "    return mean_acc\n",
    "\n",
    "\n",
    "def Q1_b(X, y, X_train_a, X_test_a, y_train_a, y_test_a):  \n",
    "    random_sizes = [50, 100, 200, 300, 400, 500, 600, 700, 800, 900]\n",
    "    ## 1- DT, 2-  RF, 3-AB, 4-LR, 5-MLP, 6-SVM\n",
    "    DT_acc_list = []\n",
    "    RF_acc_list = []\n",
    "    AB_acc_list = []\n",
    "    LR_acc_list = []\n",
    "    MLP_acc_list = []\n",
    "    SVM_acc_list = []\n",
    "    for r_size in random_sizes:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_train_a, y_train_a, train_size = r_size/len(X_train_a), random_state=45)\n",
    "#         print(\"==========\", len(X_train), len(X_test_a))\n",
    "        DT_clf = DecisionTreeClassifier().fit(X_train,y_train)\n",
    "        RF_clf = RandomForestClassifier().fit(X_train,y_train)\n",
    "        AB_clf = AdaBoostClassifier().fit(X_train,y_train)\n",
    "        LR_clf = LogisticRegression().fit(X_train,y_train)\n",
    "        MLP_clf = MLPClassifier().fit(X_train,y_train)\n",
    "        SVM_clf = SVC().fit(X_train,y_train)\n",
    "        \n",
    "        acc_1 = avg_accuracy(DT_clf, X_test_a, y_test_a)\n",
    "        acc_2 = avg_accuracy(RF_clf, X_test_a, y_test_a)\n",
    "        acc_3 = avg_accuracy(AB_clf, X_test_a, y_test_a)\n",
    "        acc_4 = avg_accuracy(LR_clf, X_test_a, y_test_a)\n",
    "        acc_5 = avg_accuracy(MLP_clf, X_test_a, y_test_a)\n",
    "        acc_6 = avg_accuracy(SVM_clf, X_test_a, y_test_a)\n",
    "\n",
    "        DT_acc_list.append(acc_1)\n",
    "        RF_acc_list.append(acc_2)\n",
    "        AB_acc_list.append(acc_3)\n",
    "        LR_acc_list.append(acc_4)\n",
    "        MLP_acc_list.append(acc_5)\n",
    "        SVM_acc_list.append(acc_6)\n",
    "    # random_sizes =1000 \n",
    "    DT_clf = DecisionTreeClassifier().fit(X_train_a,y_train_a)\n",
    "    RF_clf = RandomForestClassifier().fit(X_train_a,y_train_a)\n",
    "    AB_clf = AdaBoostClassifier().fit(X_train_a,y_train_a)\n",
    "    LR_clf = LogisticRegression().fit(X_train_a,y_train_a)\n",
    "    MLP_clf = MLPClassifier().fit(X_train_a,y_train_a)\n",
    "    SVM_clf = SVC().fit(X_train_a,y_train_a)\n",
    "    \n",
    "    acc_1 = avg_accuracy(DT_clf, X_test_a, y_test_a)\n",
    "    acc_2 = avg_accuracy(RF_clf, X_test_a, y_test_a)\n",
    "    acc_3 = avg_accuracy(AB_clf, X_test_a, y_test_a)\n",
    "    acc_4 = avg_accuracy(LR_clf, X_test_a, y_test_a)\n",
    "    acc_5 = avg_accuracy(MLP_clf, X_test_a, y_test_a)\n",
    "    acc_6 = avg_accuracy(SVM_clf, X_test_a, y_test_a)\n",
    "    \n",
    "    DT_acc_list.append(acc_1)\n",
    "    RF_acc_list.append(acc_2)\n",
    "    AB_acc_list.append(acc_3)\n",
    "    LR_acc_list.append(acc_4)\n",
    "    MLP_acc_list.append(acc_5)\n",
    "    SVM_acc_list.append(acc_6)\n",
    "    \n",
    "    random_sizes.append(1000)\n",
    "    x = range(len(random_sizes))\n",
    "    plt.figure(figsize=(9,6))\n",
    "    plt.title(\"Question1_b\")\n",
    "    plt.plot(x, DT_acc_list, color='blue', label=\"Decision Tree\")\n",
    "    plt.plot(x, RF_acc_list, color='Orange', label=\"Random Forest\")\n",
    "    plt.plot(x, AB_acc_list, color='Lime', label=\"AdaBoost\")\n",
    "    plt.plot(x, LR_acc_list, color='Red', label=\"Logistic Regression\")\n",
    "    plt.plot(x, MLP_acc_list, color='DarkRed', label=\"Neural Network\")\n",
    "    plt.plot(x, SVM_acc_list, color='Chocolate', label=\"SVM\")\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.4,linestyle=':')\n",
    "    plt.xlabel(\"train data size\")\n",
    "    plt.xticks(x, random_sizes, rotation=0)\n",
    "    plt.ylabel(\"average accuracy\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q1_c(X, y, X_train_a, X_test_a, y_train_a, y_test_a):\n",
    "    X_train_a = np.concatenate((X_train_a,[[-1.2076264, 1.20688728]]))\n",
    "    y_train_a = np.concatenate((y_train_a, [1]))\n",
    "    random_sizes = [50, 100, 200, 300, 400, 500, 600, 700, 800, 900,1000]\n",
    "    ## 1- DT, 2-  RF, 3-AB, 4-LR, 5-MLP, 6-SVM\n",
    "    DT_acc_time = []\n",
    "    RF_acc_time = []\n",
    "    AB_acc_time = []\n",
    "    LR_acc_time = []\n",
    "    MLP_acc_time = []\n",
    "    SVM_acc_time = []\n",
    "    for r_size in random_sizes:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_train_a, y_train_a, train_size = r_size/len(X_train_a), random_state=45)\n",
    "        \n",
    "        start_1 = time.time()\n",
    "        DT_clf = DecisionTreeClassifier().fit(X_train,y_train)\n",
    "        acc_1 = avg_accuracy(DT_clf, X_test, y_test)\n",
    "        end_1 = time.time()\n",
    "        time_1 = math.log(end_1 - start_1)\n",
    "        \n",
    "        start_2 = time.time()\n",
    "        RF_clf = RandomForestClassifier().fit(X_train,y_train)\n",
    "        acc_2 = avg_accuracy(RF_clf, X_test, y_test)\n",
    "        end_2 = time.time()\n",
    "        time_2 = math.log(end_2 - start_2)\n",
    "        \n",
    "        start_3 = time.time()\n",
    "        AB_clf = AdaBoostClassifier().fit(X_train,y_train)\n",
    "        acc_3 = avg_accuracy(AB_clf, X_test, y_test)\n",
    "        end_3 = time.time()\n",
    "        time_3 = math.log(end_3 - start_3)\n",
    "\n",
    "        start_4 = time.time()\n",
    "        LR_clf = LogisticRegression().fit(X_train,y_train)\n",
    "        acc_4 = avg_accuracy(LR_clf, X_test, y_test)\n",
    "        end_4 = time.time()\n",
    "        time_4 = math.log(end_4 - start_4)\n",
    "        \n",
    "        start_5 = time.time()\n",
    "        MLP_clf = MLPClassifier().fit(X_train,y_train)\n",
    "        acc_5 = avg_accuracy(MLP_clf, X_test, y_test)\n",
    "        end_5 = time.time()\n",
    "        time_5 = math.log(end_5 - start_5)\n",
    "        \n",
    "        start_6 = time.time()\n",
    "        SVM_clf = SVC().fit(X_train,y_train)\n",
    "        acc_6 = avg_accuracy(SVM_clf, X_test, y_test)\n",
    "        end_6 = time.time()\n",
    "        time_6 = math.log(end_6 - start_6)\n",
    "        \n",
    "        DT_acc_time.append(time_1)\n",
    "        RF_acc_time.append(time_2)\n",
    "        AB_acc_time.append(time_3)\n",
    "        LR_acc_time.append(time_4)\n",
    "        MLP_acc_time.append(time_5)\n",
    "        SVM_acc_time.append(time_6)\n",
    "    x = range(len(random_sizes))\n",
    "    plt.figure(figsize=(9,6))\n",
    "    plt.title(\"Question1_c\")\n",
    "    plt.plot(x, DT_acc_time, color='blue', label=\"Decision Tree\")\n",
    "    plt.plot(x, RF_acc_time, color='Orange', label=\"Random Forest\")\n",
    "    plt.plot(x, AB_acc_time, color='Lime', label=\"AdaBoost\")\n",
    "    plt.plot(x, LR_acc_time, color='Red', label=\"Logistic Regression\")\n",
    "    plt.plot(x, MLP_acc_time, color='DarkRed', label=\"Neural Network\")\n",
    "    plt.plot(x, SVM_acc_time, color='Chocolate', label=\"SVM\")\n",
    "    # Show Legend\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.4,linestyle=':')\n",
    "    plt.xlabel(\"train data size\")\n",
    "    plt.xticks(x, random_sizes, rotation=0)\n",
    "    plt.ylabel(\"Log of time\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q1_d(X_train_d, X_test_d, y_train_d, y_test_d ):\n",
    "    DT_clf = DecisionTreeClassifier(random_state=0).fit(X_train_d, y_train_d)\n",
    "    \n",
    "    y_predict_train = DT_clf.predict(X_train_d)\n",
    "    acc_train = accuracy_score(y_train_d , y_predict_train)\n",
    "    print(f\"The train accuracy of the Decision Tree model is {acc_train:.3f} .\")\n",
    "    \n",
    "    y_predict_test = DT_clf.predict(X_test_d)\n",
    "    acc_test = accuracy_score(y_test_d , y_predict_test)\n",
    "    print(f\"The test accuracy of the Decision Tree model is {acc_test:.3f} .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q1_e(X_train_d, X_test_d, y_train_d, y_test_d):\n",
    "    #min_sample_leaf = range(2,131)\n",
    "    train_auc = []\n",
    "    test_auc = []\n",
    "    for k in range(2,131):\n",
    "        \n",
    "        DT_clf = DecisionTreeClassifier(min_samples_leaf = k, random_state=0)\n",
    "        DT_clf.fit(X_train_d, y_train_d)\n",
    "        \n",
    "        y_predict_train = DT_clf.predict_proba(X_train_d)[:,1]\n",
    "#         y_predict_train = y_predict_train.max(axis=1)\n",
    "        \n",
    "        y_train_auc = metrics.roc_auc_score(y_train_d, y_predict_train)\n",
    "        train_auc.append(y_train_auc)\n",
    "        \n",
    "        y_predict_test = DT_clf.predict_proba(X_test_d)[:,1]\n",
    "#         y_predict_test = y_predict_test.max(axis=1)\n",
    "        \n",
    "        y_test_auc = metrics.roc_auc_score(y_test_d, y_predict_test)\n",
    "        test_auc.append(y_test_auc)\n",
    "    \n",
    "    min_sample_leaf_range = range(2, 131)\n",
    "    \n",
    "    min_size = [10, 20, 30,50, 80 ,100, 120]\n",
    "    plt.figure(figsize=(15,10))\n",
    "    plt.title(\"Question1_e\")\n",
    "    plt.plot(min_sample_leaf_range, train_auc, color='blue', label=\"Train datasets\")\n",
    "    plt.plot(min_sample_leaf_range, test_auc, color='Red', label=\"Test datasets\")\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.4,linestyle=':')\n",
    "    plt.xlabel(\"min_samples_leaf\")\n",
    "    plt.xticks(min_sample_leaf_range, min_sample_leaf_range, rotation=45)\n",
    "    plt.ylabel(\"AUC score\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q1_f_10_cross_validation(X_train_d, X_test_d, y_train_d, y_test_d):\n",
    "    auc_dict = {}\n",
    "    for k in range(2,96):\n",
    "        auc_dict[k]=[] \n",
    "        for i in range(10):\n",
    "            # 1 fold serve as test_data\n",
    "            X_test = X_train_d[i*100: (i+1)*100]\n",
    "            y_test = y_train_d[i*100: (i+1)*100]\n",
    "            \n",
    "            # rest 9 folds serve as train_data\n",
    "            X_train_1 = X_train_d[0: i*100]\n",
    "            X_train_2 = X_train_d[(i+1)*100: len(X_train_d)]\n",
    "            X_train = np.concatenate((X_train_1,X_train_2))\n",
    "            # generate: y_train\n",
    "            y_train_1 = y_train_d[0: i*100]\n",
    "            y_train_2 = y_train_d[(i+1)*100: len(X_train_d)]\n",
    "            y_train = np.concatenate((y_train_1,y_train_2))\n",
    "            \n",
    "            DT_clf = DecisionTreeClassifier(min_samples_leaf = k, random_state=0)\n",
    "            DT_clf.fit(X_train, y_train)\n",
    "            y_predict = DT_clf.predict_proba(X_test)[:,1]\n",
    "#             y_predict =y_predict.max(axis=1)\n",
    "            auc_score = metrics.roc_auc_score(y_test, y_predict)\n",
    "            auc_dict[k].append(auc_score)\n",
    "    k_list = []\n",
    "    auc_list=[]\n",
    "    for item in auc_dict.keys():\n",
    "        k_list.append(item)\n",
    "    for item in auc_dict.values():\n",
    "        auc_list.append(item)\n",
    "    x= range(2,96)\n",
    "    plt.figure(figsize=(15,10))\n",
    "    plt.title(\"Question1_f_10_crossValidation\")\n",
    "    plt.boxplot(auc_list)\n",
    "    plt.grid(alpha=0.4,linestyle=':')\n",
    "    plt.xlabel(\"min_samples_leaf\")\n",
    "    plt.xticks(x, x, rotation=45)\n",
    "    plt.ylabel(\"AUC score\")\n",
    "    plt.show()\n",
    "    \n",
    "    # let the min_samples_leaf = 24, can get highest CV score\n",
    "    DT_clf = DecisionTreeClassifier(min_samples_leaf = 24, random_state=0)\n",
    "    DT_clf.fit(X_train_d, y_train_d)\n",
    "    \n",
    "    y_predict_test = DT_clf.predict(X_test_d)\n",
    "    acc_test = accuracy_score(y_test_d, y_predict_test)\n",
    "    \n",
    "    y_predic_train = DT_clf.predict(X_train_d)\n",
    "    acc_train = accuracy_score(y_train_d, y_predic_train)\n",
    "    \n",
    "    print(f\"Train accuacy = {acc_train:.3f}.\")\n",
    "    print(f\"Test accuracy = {acc_test:.3f}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q1_g_GridSearchCV(X_train_d, X_test_d, y_train_d, y_test_d):\n",
    "    params={'min_samples_leaf': [x for x in range(2,96)]}\n",
    "    DT_clf = DecisionTreeClassifier(random_state=0)\n",
    "    grid = GridSearchCV(DT_clf, params, cv=10, scoring=\"roc_auc\")\n",
    "    grid.fit(X_train_d, y_train_d)\n",
    "    print(\"The optimal min_samples_leaf chosen by GridSearchCV is \", grid.best_params_)\n",
    "    \n",
    "    \n",
    "    DT_clf_g = DecisionTreeClassifier(min_samples_leaf = 28, random_state=0)\n",
    "    DT_clf_g.fit(X_train_d, y_train_d)\n",
    "    \n",
    "    y_predict_test = DT_clf_g.predict(X_test_d)\n",
    "    acc_test = accuracy_score(y_test_d, y_predict_test)\n",
    "    \n",
    "    y_predic_train = DT_clf_g.predict(X_train_d)\n",
    "    acc_train = accuracy_score(y_train_d, y_predic_train)\n",
    "    \n",
    "    print(f\"Train accuacy = {acc_train:.3f}.\")\n",
    "    print(f\"Test accuracy = {acc_test:.3f}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final weight vector is [ 3.         -0.62       -0.654       0.07793276].\n"
     ]
    }
   ],
   "source": [
    "def Question2_a_b_plot(X, y, title, feature1, feature2):\n",
    "    X = np.array(X)\n",
    "    y= np.array(y)\n",
    "\n",
    "    fig = plt.figure(figsize=(8,6))\n",
    "    plt.scatter(X[:,0], X[:,1], marker='o', c=y, cmap='rainbow')#画散点图\n",
    "    plt.title(title)#设置标题\n",
    "    plt.xlabel(f\"1st feature:  {feature1}\")#设置x轴标签\n",
    "    plt.ylabel(f\"2nd feature:  {feature2}\")#设置y轴标签\n",
    "    plt.show()#显示所画的图\n",
    "\n",
    "def Question2_a_b(X, y):\n",
    "    X = np.array(X)\n",
    "    y= np.array(y)\n",
    "    Question2_a_b_plot(X, y, title=\"Before kernel trick\", feature1 = \"x1\", feature2 =\"x2\")\n",
    "    \n",
    "    x1 = X[:,0]\n",
    "    x2 = X[:,1]\n",
    "    X_b1 = X\n",
    "    X_b1[:,0] = x1*x1\n",
    "    X_b1[:,1] = x2*x2\n",
    "    Question2_a_b_plot(X_b1, y, title=\"polynomial kernel m=0, d=2\", feature1 = \"x1^2\", feature2 =\"x2^2\")\n",
    "    \n",
    "    X_b2 = X\n",
    "    X_b2[:,0] = x1*x1\n",
    "    X_b2[:,1] = pow(2,0.5)*x1*x2\n",
    "    Question2_a_b_plot(X_b2, y, title=\"polynomial kernel m=0, d=2\", feature1 = \"x1^2\", feature2 =\"sqrt(2)*x1*x2\")\n",
    "    \n",
    "    X_b3 = X\n",
    "    X_b3[:,0] = x2*x2\n",
    "    X_b3[:,1] = pow(2,0.5)*x1*x2\n",
    "    Question2_a_b_plot(X_b3, y, title=\"polynomial kernel m=0, d=2\", feature1 = \"x2^2\", feature2 =\"sqrt(2)*x1*x2\")\n",
    "    \n",
    "def Question2_c(X, y):\n",
    "    column =np.array ([0,0,0,0,0,0,0,0])\n",
    "    X = np.column_stack((X,column))\n",
    "    x1 = X[:,0]\n",
    "    x2 = X[:,1]\n",
    "\n",
    "    X[:,2] = pow(2,0.5)*x1*x2\n",
    "    X[:,0] = x1*x1\n",
    "    X[:,1] = x2*x2\n",
    "\n",
    "    w = np.array([1,1,1,1],dtype = 'float64')\n",
    "    # learning rate = 0.2\n",
    "    lr = 0.2\n",
    "\n",
    "    converged = 0\n",
    "    m= 1\n",
    "    iteration =  []\n",
    "    iteration.append(0)\n",
    "\n",
    "    weight_total = np.array([[1,1,1,1]],dtype = 'float64')\n",
    "    while converged == 0:\n",
    "        converged = 1\n",
    "        for i in range(8):\n",
    "            w0 = w[0]\n",
    "            w1 = w[1]\n",
    "            w2 = w[2]\n",
    "            w3 = w[3]\n",
    "            y_ = w0 + w1*X[:,0][i] + w2*X[:,1][i]+ w3*X[:,2][i] \n",
    "            signal = y_*y[i]\n",
    "            if signal <= 0:\n",
    "                w[0] = w0 + (y[i]*0.2*1)\n",
    "                w[1] = w1 + (y[i]*0.2*X[:,0][i])\n",
    "                w[2] = w2 + (y[i]*0.2*X[:,1][i])\n",
    "                w[3] = w3 + (y[i]*0.2*X[:,2][i])\n",
    "                converged = 0\n",
    "                iteration.append(m)\n",
    "                weight_total = np.concatenate((weight_total,[w]))\n",
    "                m = m+1\n",
    "    print(f\"The final weight vector is {w}.\")\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    X, y = create_dataset()\n",
    "    \n",
    "    # split the dataset, train=0.8; test = 0.2\n",
    "    X_train_a, X_test_a, y_train_a, y_test_a = train_test_split(X, y, train_size=0.8, random_state=45)\n",
    "    \n",
    "#     Q1_a(X, X_train_a, X_test_a, y_train_a, y_test_a)\n",
    "#     Q1_b(X, y, X_train_a, X_test_a, y_train_a, y_test_a)\n",
    "#     Q1_c(X, y, X_train_a, X_test_a, y_train_a, y_test_a)\n",
    "    \n",
    "    # Q1_d - g: focus on the DecisionTrreeClassifier\n",
    "    X_d , y_d = create_dataset(n=2000, nf=20, nr=12, ni=8, random_state=25)\n",
    "    X_train_d, X_test_d, y_train_d, y_test_d = train_test_split(X_d , y_d ,train_size=0.5, random_state=15)\n",
    "\n",
    "#     Q1_d(X_train_d, X_test_d, y_train_d, y_test_d)\n",
    "#     Q1_e(X_train_d, X_test_d, y_train_d, y_test_d)\n",
    "#     Q1_f_10_cross_validation(X_train_d, X_test_d, y_train_d, y_test_d)\n",
    "#     Q1_g_GridSearchCV(X_train_d, X_test_d, y_train_d, y_test_d)\n",
    "\n",
    "    \n",
    "    #Question2\n",
    "    X_q2 = [[-0.8, 1],\n",
    "     [3.9, 0.4],\n",
    "     [1.4, 1],\n",
    "     [0.1, -3.3],\n",
    "     [1.2, 2.7],\n",
    "     [-2.45, 0.1],\n",
    "     [-1.5, -0.5],\n",
    "     [1.2, -1.5]]\n",
    "    y_q2= [1,-1,1,-1,-1,-1,1,1]\n",
    "    \n",
    "#     Question2_a_b(X_q2,y_q2)\n",
    "    \n",
    "    \n",
    "    ## question2_c perceptron learning algorithm\n",
    "    Question2_c(X_q2, y_q2)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
