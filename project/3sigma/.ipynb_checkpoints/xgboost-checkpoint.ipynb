{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "fjGJTVNrAlO5"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import poisson\n",
    "\n",
    "import pylab\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def delete_outliers(X_csv, y_csv):\n",
    "    \n",
    "    samples, features = X_csv.shape\n",
    "\n",
    "    del_outliers = []\n",
    "    \n",
    "    for n in range(features):\n",
    "        \n",
    "        data_cut =X_csv[n]\n",
    "        outliers = []\n",
    "#     3 sigma 数学分布\n",
    "        mean= data_cut.mean()\n",
    "        std = data_cut.std()\n",
    "\n",
    "        upper = mean + 8*std\n",
    "        lower = mean  - 8*std\n",
    "\n",
    "\n",
    "        for i,m  in enumerate(data_cut.index):\n",
    "            if data_cut[i]< lower or data_cut[i]>upper:\n",
    "                outliers.append(i)\n",
    "        del_outliers = del_outliers + outliers\n",
    "    \n",
    "    del_outliers = list(set(del_outliers))\n",
    "    \n",
    "    X_train_1 = X_csv.drop(X_csv.index[del_outliers]).reset_index(drop=True)\n",
    "    y_train_1 = y_csv.drop(y_csv.index[del_outliers]).reset_index(drop=True)\n",
    "    \n",
    "    return X_train_1, y_train_1\n",
    "    \n",
    "\n",
    "\n",
    "def cut_dim_by_2_features_corr(X_tain_clean, y_tain_clean):\n",
    "    delete_columns_dist = {}\n",
    "    n = X_tain_clean.shape[1]\n",
    "    \n",
    "    for i in range(n):\n",
    "        delete_columns_dist[i] = []\n",
    "        for j in range(i + 1, n):\n",
    "            corr_i_j = stats.pearsonr(X_tain_clean[i], X_tain_clean[j])\n",
    "            if corr_i_j[0] >= 0.96:\n",
    "                delete_columns_dist[i].append(j)\n",
    "    \n",
    "    del_columns = []\n",
    "    for key, values in delete_columns_dist.items():\n",
    "        del_columns = del_columns + values\n",
    "        \n",
    "    del_columns = list(set(del_columns))\n",
    "    return del_columns\n",
    "\n",
    "\n",
    "def cut_dim_by_features_corr_label(X_csv, y_csv, del_1st_dims):\n",
    "    X_csv = pd.DataFrame(X_csv)\n",
    "    \n",
    "    X_del_dims_1st = X_csv.drop(columns=X_csv.columns[del_1st_dims])\n",
    "    X_re_fes = pd.DataFrame(X_del_dims_1st)\n",
    "    X_re_fes['label'] = y_csv \n",
    "    index_corrScore_xy = {}\n",
    "\n",
    "    for i, true_index in enumerate(X_re_fes.columns):\n",
    "        if i == len(X_re_fes.columns) - 1:\n",
    "            break\n",
    "            \n",
    "        a = X_re_fes.iloc[:, i]\n",
    "        b = X_re_fes.iloc[:, -1]\n",
    "        \n",
    "        corr_ab = stats.pearsonr(a, b)[0]\n",
    "        corr_ab = round(corr_ab, 7)\n",
    "        \n",
    "        index_corrScore_xy[true_index] = corr_ab\n",
    "    del_2nd_index = []\n",
    "    \n",
    "    for key, value in index_corrScore_xy.items():\n",
    "        if - 0.15 < index_corrScore_xy[key] < 0.15:\n",
    "            del_2nd_index.append(key)\n",
    "    \n",
    "    del_2nd_index = list(set(del_2nd_index))\n",
    "    return del_2nd_index\n",
    "\n",
    "def filter_features(X_train_1, y_train_1):\n",
    "    del_1st_dims = cut_dim_by_2_features_corr(X_train_1, y_train_1)\n",
    "    del_2nd_dims = cut_dim_by_features_corr_label(X_train_1, y_train_1, del_1st_dims)\n",
    "    del_dims = del_1st_dims + del_2nd_dims\n",
    "    del_dims = [int (i) for i in del_dims]\n",
    "    return del_dims\n",
    "\n",
    "\n",
    "def del_features(X, y, del_features):\n",
    "    X = pd.DataFrame(X)\n",
    "    y = pd.DataFrame(y)\n",
    "    X = X.drop(columns = X.columns[del_features]).T.reset_index(drop=True).T\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fZh5hvlcMV__"
   },
   "source": [
    "# Xgboost使用sklearn接口的分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "lNUDI5_1nBof"
   },
   "outputs": [],
   "source": [
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "clf = XGBClassifier(\n",
    "    \n",
    "    silent=0,  #设置成1则没有运行信息输出，最好是设置为0.是否在运行升级时打印消息。\n",
    "    learning_rate=0.01,\n",
    "\n",
    "    n_estimators=1000,\n",
    "\n",
    "    max_depth= 7,     # 这参数（3-10），构建树的深度，越大越容易过拟合\n",
    "    min_child_weight = 1,\n",
    "\n",
    "    \n",
    "    gamma = 0.1,  \n",
    "\n",
    "    \n",
    "    colsample_bytree=0.9, \n",
    "    subsample=0.8,   \n",
    "    \n",
    "    # max_delta_step=0,       # 最大增量步长，我们允许每个树的权重估计\n",
    "\n",
    "    #objective= 'multi:softmax', #多分类的问题 指定学习任务和相应的学习目标\n",
    "    objective= 'multi:softmax',\n",
    "\n",
    "    reg_alpha=0.01,  # L1 正则项参数\n",
    "\n",
    "    # nthread=4,   # CPU 线程数 默认最大\n",
    "\n",
    "    scale_pos_weight=1,   # 如果取值大于0的话，在类别样本不平衡的情况下有助于快速收敛，平衡正负权重\n",
    "    seed = 20\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "S__L_3QIne1Y"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "X_train = pd.read_csv(\"../MLinTheUnknown-Data/X_train.csv\", header=None)\n",
    "y_train = pd.read_csv(\"../MLinTheUnknown-Data/y_train.csv\", header=None)\n",
    "X_val = pd.read_csv(\"../MLinTheUnknown-Data/X_val.csv\", header=None)\n",
    "y_val = pd.read_csv(\"../MLinTheUnknown-Data/y_val.csv\", header=None)\n",
    "X_test = pd.read_csv(\"../MLinTheUnknown-Data/X_test.csv\", header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vI-6MfScMbWm"
   },
   "source": [
    "# PCA--降维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "xxy1Yp1CDF-7"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA, IncrementalPCA\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# pca = PCA(n_components=11)\n",
    "# X_train = pca.fit_transform (X_train)\n",
    "# X_val = pca.transform (X_val)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# del_dims = filter_features(X_trfit_transformain, y_train)\n",
    "\n",
    "# X_train, y_train = del_features(X_train,y_train, del_dims)\n",
    "# X_val,  y_val  = del_features(X_val,y_val, del_dims)\n",
    "\n",
    "\n",
    "# step2, del the outliers noise\n",
    "# X_train = pd.DataFrame(X_train)\n",
    "\n",
    "# X_train, y_train = delete_outliers(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "X_val = np.array(X_val)\n",
    "\n",
    "y_train =np.array(y_train).ravel()\n",
    "y_val =np.array(y_val).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nAtbru_bMe0E"
   },
   "source": [
    "#Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "kY1A2rjGEHge"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "\n",
    "# min_max_scaler = preprocessing.MinMaxScaler()\n",
    "# X_train = min_max_scaler.fit_transform(X_train)\n",
    "# X_val = min_max_scaler.transform(X_val)\n",
    "\n",
    "# normalizer = Normalizer()\n",
    "# X_train = normalizer.fit_transform(X_train)\n",
    "# X_val = normalizer.transform(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YhVDTGtCZUJg"
   },
   "source": [
    "# hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tKZPItwWerAa"
   },
   "source": [
    "## step1: max_depth和min_weight参数调优"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "xpFz-g7AZrmk"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/COMP9444/lib/python3.7/site-packages/sklearn/model_selection/_split.py:297: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# param_grid\n",
    "# param_distributions\n",
    "\n",
    "params_test = False\n",
    "\n",
    "kf = KFold( shuffle = False, random_state=0)\n",
    "\n",
    "if params_test == True:\n",
    "    param_test1 = {\n",
    "    'max_depth':range(3,11,2),\n",
    "    'min_child_weight':range(1,6,2)\n",
    "    }\n",
    "\n",
    "    gsearch1 = GridSearchCV(\n",
    "        estimator = XGBClassifier(), \n",
    "        param_grid = param_test1,\n",
    "\n",
    "        scoring='f1_macro',\n",
    "        n_jobs=-1,\n",
    "        cv= kf,\n",
    "        verbose=3\n",
    "       \n",
    "    )\n",
    "\n",
    "    gsearch1.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    print(gsearch1.best_params_)\n",
    "    print()\n",
    "    print(gsearch1.best_score_)\n",
    "    print()\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0RzfABfYes9h"
   },
   "source": [
    "## step2: gamma  和 n_estimators参数调优"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "hWH1eJ8_b9JC"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "Gamma =False\n",
    "\n",
    "if Gamma== True:\n",
    "    param_test2 = {\n",
    "        'gamma':[i/10.0 for i in range(0,5)],\n",
    "        }\n",
    "    gsearch2 = GridSearchCV(\n",
    "        estimator = XGBClassifier(),              \n",
    "        param_grid = param_test2, \n",
    "\n",
    "        scoring='f1_macro',\n",
    "        n_jobs=-1,\n",
    "        cv= kf,\n",
    "        verbose=3\n",
    "        )\n",
    "    gsearch2.fit(X_train, y_train)\n",
    "    print(gsearch2.best_params_)\n",
    "    print()\n",
    "    print(gsearch2.best_score_)\n",
    "    print()\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ecPRXQO_hQWb"
   },
   "source": [
    "## step3: 调整subsample 和 colsample_bytree参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "CjyH7JMHZcrU"
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "punning_params4= False\n",
    "if punning_params4==True:\n",
    "    param_test3 = {\n",
    "        'subsample':[i/10.0 for i in range(6,10)],\n",
    "        'colsample_bytree':[i/10.0 for i in range(6,10)],\n",
    "        }\n",
    "    \n",
    "    gsearch3 = GridSearchCV(\n",
    "        estimator = XGBClassifier(),   \n",
    "        param_grid = param_test3, \n",
    "        scoring='f1_macro',\n",
    "        n_jobs=-1,\n",
    "        cv= kf,\n",
    "        verbose=3\n",
    "        )\n",
    "\n",
    "    gsearch3.fit(X_train, y_train)\n",
    "    print(gsearch3.best_params_)\n",
    "    print()\n",
    "    print(gsearch3.best_score_)\n",
    "    print()\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJv24F3m4OOQ"
   },
   "source": [
    "## step4:  基于以上，单独调整 'n_estimators': \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "tWth3xet4Rif",
    "outputId": "462f04ff-acf5-42c5-e10b-a6d2b44458b7"
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "N_estimators = False\n",
    "if N_estimators == True:\n",
    "    param_test4 = {\n",
    "          'n_estimators': range(200, 2000, 100)\n",
    "          }\n",
    "    gsearch4 = GridSearchCV(\n",
    "          estimator = XGBClassifier(),   \n",
    "          param_grid = param_test4, \n",
    "\n",
    "\n",
    "          scoring='f1_macro',\n",
    "          n_jobs=-1,\n",
    "          cv= kf,\n",
    "          verbose=3\n",
    "\n",
    "          )\n",
    "\n",
    "    gsearch4.fit(X_train, y_train)\n",
    "    print(gsearch4.best_params_)\n",
    "    print()\n",
    "    print(gsearch4.best_score_)\n",
    "    print()\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nse2MB6ehpzm"
   },
   "source": [
    "## step5: 正则化参数调优  and n_estimators if necessary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "GaeS_GY3h7Pe"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "RE = False\n",
    "if RE == True:\n",
    "    param_test5 = {\n",
    "        'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100],\n",
    "        }\n",
    "    gsearch5 = GridSearchCV(\n",
    "        estimator = XGBClassifier(),  \n",
    "        param_grid = param_test5, \n",
    "\n",
    "        scoring='f1_macro',\n",
    "        n_jobs=-1,\n",
    "        cv= kf,\n",
    "        verbose=3\n",
    "        )\n",
    "\n",
    "    gsearch5.fit(X_train, y_train)\n",
    "    print(gsearch5.best_params_)\n",
    "    print()\n",
    "    print(gsearch5.best_score_)\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O7SF-jQP6S80"
   },
   "source": [
    "## step6: seed 和 objective的选择"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "xYkZoW-C5X0b"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import numpy as np \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "SEED = False\n",
    "if SEED == True:\n",
    "    param_test6 ={\n",
    "        'seed': np.arange(20, 400, step =50),\n",
    "        'objective': ['multi:softmax','binary:logistic']\n",
    "    }\n",
    "\n",
    "    gsearch6 = GridSearchCV(\n",
    "        estimator = XGBClassifier(),\n",
    "        param_grid = param_test6,\n",
    "\n",
    "        cv= kf,\n",
    "        scoring='f1_macro',\n",
    "        verbose=3,\n",
    "        n_jobs=-1,  \n",
    "    )\n",
    "    \n",
    "    gsearch6.fit(X_train, y_train)\n",
    "    print(gsearch6.best_params_)\n",
    "    print()\n",
    "    print(gsearch6.best_score_)\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z4qL7XKz37-T"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T1BiSJvWiPwU"
   },
   "source": [
    "## step7: 降低学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "x8ofmCP8iTDF"
   },
   "outputs": [],
   "source": [
    "# adjust_lr = True\n",
    "# if adjust_lr == True:\n",
    "\n",
    "#     xgb_lr = XGBClassifier()\n",
    "\n",
    "#     modelfit(xgb_lr, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sg9fWaCNiTHq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k4hFuLf0Mk0L"
   },
   "source": [
    "# K-fold for train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "FLYeIDETDGBr"
   },
   "outputs": [],
   "source": [
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "K_fold_train =False\n",
    "if K_fold_train == True:\n",
    "    kf = KFold( shuffle = True, random_state=0)\n",
    "    f1_score_list = []\n",
    "    X_train = pd.DataFrame(X_train)\n",
    "    y_train = pd.DataFrame(y_train)\n",
    "\n",
    "    for train_idx, test_idx in kf.split(X_train):\n",
    "        model = xgb.XGBClassifier(\n",
    "            max_depth=7,\n",
    "            learning_rate=0.1,\n",
    "            n_estimators=1000,\n",
    "            gamma = 0.1,  \n",
    "            silent=True,\n",
    "            objective='multi:softmax',\n",
    "            colsample_bytree=0.9, \n",
    "            subsample=0.8,\n",
    "            seed=20,\n",
    "            num_class = 6\n",
    "        )\n",
    "\n",
    "        train_X = X_train.loc[train_idx]\n",
    "        train_X =np.array(train_X)\n",
    "\n",
    "        train_y = y_train.loc[train_idx]\n",
    "        train_y =np.array(train_y).ravel()\n",
    "\n",
    "        val_X = X_train.loc[test_idx]\n",
    "        val_X = np.array(val_X)\n",
    "\n",
    "        val_y = y_train.loc[test_idx]\n",
    "        val_y = np.array(val_y).ravel()\n",
    "\n",
    "\n",
    "        model.fit(train_X,train_y)\n",
    "        pred = model.predict(val_X)\n",
    "\n",
    "        f1= f1_score(val_y,  pred,  average='weighted')\n",
    "        f1_score_list.append(f1)\n",
    "\n",
    "    print(f\"==K-Fold==f1-score: {(np.array(f1_score_list)).mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dHmPQ9RxMiOW"
   },
   "source": [
    "# validiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "4biU_txJDGE1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score(weighted) 0.9899290331038155\n",
      "f1_score(micro) 0.9899352983465133\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "Validation = True\n",
    "\n",
    "\n",
    "if Validation == True:\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train).ravel()\n",
    "\n",
    "    model = xgb.XGBClassifier(\n",
    "            max_depth=7,\n",
    "            learning_rate=0.1,\n",
    "            n_estimators=1000,\n",
    "            gamma = 0.1,  \n",
    "            silent=True,\n",
    "            objective='multi:softmax',\n",
    "            colsample_bytree=0.9, \n",
    "            subsample=0.8,\n",
    "            seed=20,\n",
    "            num_class = 6\n",
    "    )\n",
    "\n",
    "    model.fit(X_train,y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "    f1_macro = f1_score(y_val, y_pred, average='weighted')\n",
    "    print(\"f1_score(weighted)\",f1_macro)\n",
    "\n",
    "    f1_micro = f1_score(y_val, y_pred, average='micro')\n",
    "    print(\"f1_score(micro)\",f1_micro)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SdIAhBe2EZ01"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "YR4ms9NgFGdn"
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(18,18))\n",
    "# plot_importance(model)\n",
    "# # plt.savefig(\"feature_importance(f42-f65).png\")\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "xgboost.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
