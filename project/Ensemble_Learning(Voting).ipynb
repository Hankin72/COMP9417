{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ensemble Learning(Voting).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZK8k7P1cu_2"
      },
      "source": [
        "# hello"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVN2dACKc5CN",
        "outputId": "eae4310c-c53d-4178-bff8-485bf4713e40"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYOO-S5KO3ag"
      },
      "source": [
        "!pip install -q -U easydl\n",
        "from easydl import clear_output\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "\n",
        "import time\n",
        "import pylab\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.stats as stats\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.decomposition import PCA, IncrementalPCA\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBoyRCY6PwH7"
      },
      "source": [
        "X = pd.read_csv(\"/content/drive/MyDrive/Again/X_train.csv\", header=None)\n",
        "y = pd.read_csv(\"/content/drive/MyDrive/Again/y_train.csv\", header=None)\n",
        "X_val = pd.read_csv(\"/content/drive/MyDrive/Again/X_val.csv\", header=None)\n",
        "y_val = pd.read_csv(\"/content/drive/MyDrive/Again/y_val.csv\", header=None)\n",
        "X_test = pd.read_csv(\"/content/drive/MyDrive/Again/X_test.csv\", header=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kU11A1GotDL6"
      },
      "source": [
        "from sklearn import datasets, linear_model, svm\n",
        "\n",
        "def scv_classifier(X, y, X_val, y_val, X_test):   # SVM method\n",
        "    # normalization\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X)\n",
        "    y_train = np.array(y).ravel()\n",
        "    X_val = scaler.transform(X_val)\n",
        "    y_val = np.array(y_val).ravel()\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    # decomposition\n",
        "    pca = PCA(n_components=32, random_state=0)\n",
        "    X_train = pca.fit_transform(X_train)\n",
        "    X_val = pca.transform(X_val)\n",
        "    X_test = pca.transform(X_test)\n",
        "    \n",
        "    # train\n",
        "    svc = svm.SVC(C=2.0)\n",
        "    svc.fit(X_train, y_train)\n",
        "\n",
        "    y_pred_val = svc.predict(X_val)\n",
        "    y_pred_test = svc.predict(X_test)\n",
        "    print(f1_score(y_val, y_pred_val, average='weighted'))\n",
        "\n",
        "    return y_pred_val, y_pred_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvHVouSeqwif"
      },
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def Voting_RF_ETC_KNN(X, y, X_val, y_val, X_test):  # ensemble\n",
        "    # decomposition\n",
        "    pca = PCA(n_components=35, random_state=42)\n",
        "    X_train = pca.fit_transform(X)\n",
        "    X_val = pca.transform(X_val)\n",
        "\n",
        "    # normalization\n",
        "    min_max_scaler = preprocessing.MinMaxScaler()\n",
        "    X_train = min_max_scaler.fit_transform(X_train)\n",
        "    X_val = min_max_scaler.transform(X_val)\n",
        "    y_train = np.array(y).ravel()\n",
        "    y_val = np.array(y_val).ravel()\n",
        "\n",
        "    # train\n",
        "    clf_rf = RandomForestClassifier(n_estimators=160, criterion='gini', min_samples_leaf=1, max_features='auto',\n",
        "                                    min_samples_split=2, random_state=0)\n",
        "\n",
        "    clf_etc = ExtraTreesClassifier(criterion='gini', max_features='auto', random_state=0)\n",
        "\n",
        "    clf_knn = KNeighborsClassifier(n_neighbors=1, weights='uniform', leaf_size=5, algorithm='auto', p=1)\n",
        "\n",
        "    clf_rf.fit(X_train, y_train)\n",
        "    clf_etc.fit(X_train, y_train)\n",
        "    clf_knn.fit(X_train, y_train)\n",
        "\n",
        "    eclf1 = VotingClassifier(estimators=[('rf', clf_rf), ('etc', clf_etc), ('knn', clf_knn)], voting='hard')\n",
        "    eclf1.fit(X_train, y_train)\n",
        "\n",
        "    y_pre_voting = eclf1.predict(X_val)\n",
        "\n",
        "    f1_hard_voting = f1_score(y_val, y_pre_voting, average='weighted')\n",
        "\n",
        "    print(\"Hard Voting for f1 (weighted) socre:  \", f1_hard_voting)\n",
        "\n",
        "    X_test = pca.transform(X_test)\n",
        "    X_test = min_max_scaler.transform(X_test)\n",
        "    y_test_voting = eclf1.predict(X_test)\n",
        "\n",
        "    return y_pre_voting,y_test_voting"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zgShdhPrZhs"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "def lr_classfier(X, y, X_val, y_val, X_test): # LogisticRegression\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X)\n",
        "    X_val = scaler.transform(X_val)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    pca = PCA(n_components=65, random_state=0)\n",
        "\n",
        "    X_train = pca.fit_transform(X_train)\n",
        "    X_val = pca.transform(X_val)\n",
        "    X_test = pca.transform(X_test)\n",
        "\n",
        "    y_train = np.array(y).ravel()\n",
        "    y_val = np.array(y_val).ravel()\n",
        "\n",
        "    clf = LogisticRegression(C=10.1, solver='newton-cg')\n",
        "\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    pred_val_y = clf.predict(X_val)\n",
        "    pred_test_y = clf.predict(X_test)\n",
        "\n",
        "    print('f1_score is :', f1_score(y_val, pred_val_y, average='weighted'))\n",
        "\n",
        "    return pred_val_y, pred_test_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOCJB6Y9erEL"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "def ada_classifier(X, y, X_val, y_val, X_test): # Adaboost\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X)\n",
        "    y_train = np.array(y).ravel()\n",
        "    X_val = scaler.transform(X_val)\n",
        "    y_val = np.array(y_val).ravel()\n",
        "    X_test = scaler.transform(X_test)\n",
        "    # pca dims deduction\n",
        "    pca = PCA(n_components=58, random_state=0)\n",
        "    X_train = pca.fit_transform(X_train)\n",
        "    X_val = pca.transform(X_val)\n",
        "    X_test = pca.transform(X_test)\n",
        "\n",
        "    clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=12), n_estimators=150)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred_val = clf.predict(X_val)\n",
        "    y_pred_test = clf.predict(X_test)\n",
        "\n",
        "    print(\"ada_classifier = \",f1_score(y_val, y_pred_val, average='weighted'))\n",
        "    return y_pred_val, y_pred_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUiuIVKmw2Eo"
      },
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import ExtraTreeClassifier\n",
        "\n",
        "def Bagging_n_extraTree(X, y, X_val, y_val, X_test): # ExtraTree with bagging\n",
        "    \n",
        "    pca = PCA(n_components=35, random_state=42)\n",
        "    # random_state=42\n",
        "    X_train = pca.fit_transform (X)\n",
        "    X_val = pca.transform (X_val)\n",
        "\n",
        "    min_max_scaler = preprocessing.MinMaxScaler()\n",
        "    X_train = min_max_scaler.fit_transform(X_train)\n",
        "    X_val = min_max_scaler.transform(X_val)\n",
        "\n",
        "    y_train = np.array(y).ravel()\n",
        "    y_val = np.array(y_val).ravel()\n",
        "    #  the clssifier of bagging along with extratree\n",
        "    bagging_model = BaggingClassifier(\n",
        "        base_estimator= ExtraTreeClassifier(criterion = 'entropy',max_features = 'auto',random_state=0),\n",
        "        n_estimators = 170,\n",
        "        max_samples = 6400,\n",
        "        max_features = 34,\n",
        "        random_state = 20\n",
        "    )\n",
        "    \n",
        "    bagging_model.fit(X_train, y_train )\n",
        "    y_pred_bag = bagging_model.predict(X_val)\n",
        "    \n",
        "    f1_bag= f1_score(y_val, y_pred_bag, average='weighted')\n",
        "    print('f1 score ---Bagging_eTree:', f1_bag)\n",
        "\n",
        "    X_test = pca.transform (X_test)\n",
        "    X_test = min_max_scaler.transform(X_test)\n",
        "    y_test_bag = bagging_model.predict(X_test)\n",
        "  \n",
        "    return y_pred_bag, y_test_bag"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r13ekovPw2Ge"
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "def GradientBoosting(X, y, X_val, y_val, X_test): # GradientBoosting\n",
        "    # standardization \n",
        "    scaler = StandardScaler()\n",
        "    Xtrain = scaler.fit_transform(X)\n",
        "    Xval = scaler.transform(X_val)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    pca = PCA(n_components=65, random_state=0)\n",
        "    Xtrain = pca.fit_transform(Xtrain)\n",
        "    Xval = pca.transform(Xval)\n",
        "    X_test = pca.transform(X_test)\n",
        "\n",
        "    clf = GradientBoostingClassifier(n_estimators=150)\n",
        "    clf.fit(Xtrain, y)\n",
        "    pred_y_val = clf.predict(Xval)\n",
        "    pred_y_test = clf.predict(X_test)\n",
        "\n",
        "    f1 = f1_score(y_val, pred_y_val, average='weighted')\n",
        "    print(\"f1_score(weighted)of === GradientBoosting: \",f1)\n",
        "\n",
        "    return pred_y_val, pred_y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzWnIYIbw2II"
      },
      "source": [
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "def Light_gbm(X, y, X_val, y_val, X_test): # lightgbm\n",
        "    \n",
        "    pca = PCA(n_components=40, random_state=42)\n",
        "    X_train = pca.fit_transform (X)\n",
        "    X_val = pca.transform (X_val)\n",
        "\n",
        "    min_max_scaler = preprocessing.MinMaxScaler()\n",
        "    X_train = min_max_scaler.fit_transform(X_train)\n",
        "    X_val = min_max_scaler.transform(X_val)\n",
        "\n",
        "    y_train = np.array(y).ravel()\n",
        "    y_val = np.array(y_val).ravel() \n",
        "\n",
        "    gbm = LGBMClassifier(\n",
        "       num_leaves=31,\n",
        "        max_depth = -1, \n",
        "        \n",
        "        learning_rate=0.1,\n",
        "        n_estimators= 120, \n",
        "        silent = True,\n",
        "        objective = 'multiclass' )\n",
        "    \n",
        "    gbm.fit(X_train,y_train)\n",
        "    #  the best iteration is 46\n",
        "    y_pred_gbm = gbm.predict(X_val, num_iteration=46)\n",
        "    f1_gbm = f1_score(y_val, y_pred_gbm, average='weighted')\n",
        "    print(\"f1_score(weighted)of === gbm: \",f1_gbm)\n",
        "\n",
        "    X_test = pca.transform (X_test)\n",
        "    X_test = min_max_scaler.transform(X_test)\n",
        "    y_test_gbm = gbm.predict(X_test)\n",
        "\n",
        "    return y_pred_gbm, y_test_gbm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjjqTTzllZwZ"
      },
      "source": [
        "def KNN_classifier(X, y, X_val, y_val, X_test): # KNN\n",
        "    pca = PCA(n_components=35, random_state= 42)\n",
        "    X_train = pca.fit_transform (X)\n",
        "    X_val = pca.transform (X_val)\n",
        "\n",
        "    min_max_scaler = preprocessing.MinMaxScaler()\n",
        "    X_train = min_max_scaler.fit_transform(X_train)\n",
        "    X_val = min_max_scaler.transform(X_val)\n",
        "\n",
        "    y_train = np.array(y).ravel()\n",
        "    y_val = np.array(y_val).ravel()\n",
        "    \n",
        "    #KNN -classifier\n",
        "    clf_knn = KNeighborsClassifier(\n",
        "            n_neighbors = 1, \n",
        "            weights = 'uniform', \n",
        "            leaf_size =5,\n",
        "            algorithm = 'auto',\n",
        "            p=1)\n",
        "    \n",
        "    clf_knn.fit(X_train, y_train)\n",
        "    y_pred_knn= clf_knn.predict(X_val)\n",
        "    f1_knn= f1_score(y_val, y_pred_knn, average='weighted')\n",
        "\n",
        "    print('f1 score --KNN:', f1_knn)\n",
        "    print()\n",
        "\n",
        "    X_test = pca.transform (X_test)\n",
        "    X_test = min_max_scaler.transform(X_test)\n",
        "    y_test_KNN = clf_knn.predict(X_test)\n",
        "\n",
        "    return y_pred_knn,y_test_KNN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4q1y2ZcRlZ0h"
      },
      "source": [
        "def RF_n_ExtraTree_classifier(X, y, X_val, y_val, X_test): # classifiers of RandomForest and ExtraTree\n",
        "\n",
        "    pca = PCA(n_components=35, random_state= 42)\n",
        "    X_train = pca.fit_transform (X)\n",
        "    X_val = pca.transform (X_val)\n",
        "\n",
        "    # # # step3, normalization \n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    y_train = np.array(y).ravel()\n",
        "    X_val = scaler.transform(X_val)\n",
        "    y_val = np.array(y_val).ravel()\n",
        "\n",
        "    # Random forest - classifier\n",
        "    clf_rf = RandomForestClassifier(n_estimators = 160, \n",
        "                      criterion='gini', \n",
        "                      min_samples_leaf= 1,\n",
        "                      max_features='auto',\n",
        "                      min_samples_split=2, \n",
        "                      random_state=0)\n",
        "    clf_rf.fit(X_train, y_train)\n",
        "    y_pred_rf = clf_rf.predict(X_val)\n",
        "    f1= f1_score(y_val, y_pred_rf, average='weighted')\n",
        "    print(\"f1_score_Randomforest:   \",f1)\n",
        "    print()\n",
        "\n",
        "     # ExtraTree - classifier\n",
        "    clf_etc = ExtraTreesClassifier(random_state=0, \n",
        "                                  criterion = 'gini',\n",
        "                                   max_features = 'auto'\n",
        "                                  )\n",
        "    clf_etc.fit(X_train, y_train)\n",
        "    y_pred_etc = clf_etc.predict(X_val)\n",
        "\n",
        "    f1_etc= f1_score(y_val, y_pred_etc, average='weighted')\n",
        "    print(\"f1_score___ExtraTress Classifier:   \",f1_etc)\n",
        "    print()\n",
        "\n",
        "    X_test = pca.transform (X_test)\n",
        "    X_test = scaler.transform(X_test)\n",
        "    \n",
        "    y_test_RF = clf_rf.predict(X_test)\n",
        "    y_test_ETC = clf_etc.predict(X_test)\n",
        "\n",
        "    return  y_pred_rf, y_test_RF, y_pred_etc, y_test_ETC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S38BLyaolZ32"
      },
      "source": [
        "from xgboost.sklearn import XGBClassifier\n",
        "from sklearn.decomposition import PCA, IncrementalPCA\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from xgboost import plot_importance\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn import preprocessing\n",
        "\n",
        "\n",
        "def xgboost(X, y, X_val, y_val, X_test):  # XGBoost\n",
        "    pca = PCA(n_components = 40, random_state=42)\n",
        "    X_train = pca.fit_transform (X)\n",
        "    X_val = pca.transform (X_val)\n",
        "    X_train = np.array(X_train)\n",
        "    X_val = np.array(X_val)\n",
        "\n",
        "    y_train =np.array(y).ravel()\n",
        "    y_val =np.array(y_val).ravel()\n",
        "\n",
        "    # Normalization\n",
        "    min_max_scaler = preprocessing.MinMaxScaler()\n",
        "    X_train = min_max_scaler.fit_transform(X_train)\n",
        "    X_val = min_max_scaler.transform(X_val)\n",
        "\n",
        "    Validation = True\n",
        "    if Validation == True:\n",
        "        model = xgb.XGBClassifier(\n",
        "                max_depth=7,\n",
        "                learning_rate=0.05,\n",
        "                n_estimators=1000,\n",
        " \n",
        "                silent=True,\n",
        "                objective='multi:softmax',\n",
        "        )\n",
        "\n",
        "        model.fit(X_train,y_train)\n",
        "        y_pred_xgb = model.predict(X_val)\n",
        "\n",
        "        f1_xgb = f1_score(y_val, y_pred_xgb, average='weighted')\n",
        "        print(\"f1_score(weighted)___xgboost\",f1_xgb)\n",
        "\n",
        "        X_test = pca.transform (X_test)\n",
        "        X_test = min_max_scaler.transform(X_test)\n",
        "        y_test_xgb = model.predict(X_test)\n",
        "\n",
        "    return y_pred_xgb, y_test_xgb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQJCVIGLRfPd"
      },
      "source": [
        "# **Feature Engineering** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32cUmkBCRuZV"
      },
      "source": [
        "### 1 Normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pe8CK4IbVPfp"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_s = scaler.fit_transform(X)\n",
        "X_val_s = scaler.transform(X_val)\n",
        "X_test_s = scaler.transform(X_test)\n",
        "new_y = np.zeros((y.values.shape[0], 6), dtype=int)\n",
        "for i in range(y.values.shape[0]):\n",
        "  new_y[i, int(y.values.ravel()[i]) - 1] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxgun5rOSCxT"
      },
      "source": [
        "### 2 Bottleneck Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0riDji0PwJ9"
      },
      "source": [
        "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, Concatenate, Lambda, GaussianNoise, Activation\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.activations import relu, swish\n",
        "from tensorflow.keras.metrics import AUC\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxsMshIzPwMW"
      },
      "source": [
        "def AutoEndcoder(input_dim, output_dim, noise=0.05):\n",
        "    i = Input(input_dim)\n",
        "    encoded = BatchNormalization()(i)\n",
        "    encoded = GaussianNoise(noise)(encoded)\n",
        "\n",
        "    encoded = Dense(64, activation='relu')(encoded)\n",
        "    encoded = Dropout(0.15)(encoded)\n",
        "    encoded = Dense(32, activation='relu')(encoded)\n",
        "    decoded = Dropout(0.15)(encoded)\n",
        "    \n",
        "    decoded = Dense(64, activation='relu')(decoded)\n",
        "    decoded = Dropout(0.15)(decoded)\n",
        "    decoded = Dense(input_dim, name='decoded')(decoded) \n",
        "    x = Dense(64, activation='relu')(decoded)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = Dense(output_dim, activation='sigmoid', name='label_output')(x)\n",
        "\n",
        "    encoder = Model(inputs=i, outputs=encoded)\n",
        "    autoencoder = Model(inputs=i, outputs=[decoded, x]) \n",
        "\n",
        "    autoencoder.compile(optimizer=Adam(0.01), loss={'decoded':'mse', 'label_output':'binary_crossentropy'})\n",
        "\n",
        "    return autoencoder, encoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4y_V3KebPwOc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e865978-952c-405a-ea47-6b5d5fdc3b72"
      },
      "source": [
        "TRAIN = False\n",
        "SEED = 20\n",
        "tf.random.set_seed(SEED)\n",
        "autoencoder, encoder = AutoEndcoder(X_s.shape[-1], new_y.shape[-1], noise=0.05)\n",
        "if TRAIN:\n",
        "    autoencoder.fit(X_s, (X_s, new_y),\n",
        "              epochs=1000,\n",
        "              batch_size=4096, \n",
        "              validation_split=0.2,\n",
        "              callbacks=[EarlyStopping('val_loss', patience=10, restore_best_weights=True)])\n",
        "    encoder.save_weights('/content/drive/MyDrive/Again/encoder.hdf5')\n",
        "else:\n",
        "    encoder.load_weights('/content/drive/MyDrive/Again/encoder.hdf5')\n",
        "encoder.trainable = False\n",
        "print(\"SAVE/LOAD FINISH\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SAVE/LOAD FINISH\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZ7JVWRejcNL"
      },
      "source": [
        "# **Model Design and Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbYLGfFbbo9C"
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "def f1(ytruth, y_pred, average='macro'):\n",
        "  return f1_score(ytruth, np.argmax(np.array(y_pred), axis=1) + 1, average=average)\n",
        "\n",
        "def MLP_Model(input_dim, output_dim, encoder, hidden_layers, lr=0.001, smooth=0.1):\n",
        "    inputs = Input(input_dim)\n",
        "    x = encoder(inputs)  \n",
        "    x = Concatenate()([x, inputs]) \n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.1)(x)\n",
        "    \n",
        "    for layer in hidden_layers:\n",
        "        x = Dense(layer)(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Lambda(swish)(x)\n",
        "        x = Dropout(0.2)(x)\n",
        "    \n",
        "    x = Dense(output_dim, activation='sigmoid')(x)\n",
        "    model = Model(inputs=inputs, outputs=x)\n",
        "    \n",
        "    model.compile(optimizer=Adam(lr),\n",
        "                  loss=BinaryCrossentropy(label_smoothing=smooth),\n",
        "                  metrics=[tf.keras.metrics.AUC(name='auc')])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sV4-UylsuLtP"
      },
      "source": [
        "def ensemble(prediction):\n",
        "  # voting algorithm. Choose the prediction which appears mostly.\n",
        "  df = pd.DataFrame(np.stack(prediction).T)\n",
        "  maxCount = np.zeros(df.shape[0])\n",
        "  for i in range(df.shape[0]):\n",
        "      line = df.iloc[i, :]\n",
        "      count = defaultdict(int)\n",
        "      for j in range(line.shape[0]):\n",
        "          count[line[j]] += 1\n",
        "      maxList = sorted(list(count.items()), key=lambda x: x[1], reverse=True)\n",
        "      maxValue = maxList[0][0]\n",
        "      maxCount[i] = maxValue\n",
        "  return maxCount"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkRDvMKtNat8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvkBBn7ebo_T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2051247c-6594-41a4-fc61-0f155ac53b1b"
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "\n",
        "TEST = False\n",
        "TRAIN = False\n",
        "\n",
        "SEED = 20\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "hidden_layers = [512, 2048, 256, 256, 32, 32, 32]\n",
        "batchSize = 2048\n",
        "learningRate = 0.002\n",
        "laplaceSmoothing = 0.05\n",
        "\n",
        "if TEST:\n",
        "  modelList = []\n",
        "  scoreList = []\n",
        "  kf = KFold(shuffle=True, random_state=0)\n",
        "  i = 1\n",
        "  for train_index, test_index in kf.split(X_s):\n",
        "    print(f\"============= THE {i}-th FOLD ==================\")\n",
        "    i += 1\n",
        "    Xtrain, Xtest = X_s[train_index], X_s[test_index]\n",
        "    ytrain = new_y[train_index]\n",
        "    ytest = y.values[test_index]\n",
        "\n",
        "    Xtrain_not_s, Xtest_not_s = X.values[train_index], X.values[test_index]\n",
        "    ytrain_not_s = y.values[train_index]\n",
        "\n",
        "    mlp = MLP_Model(Xtrain.shape[1], ytrain.shape[1], encoder, hidden_layers, lr=learningRate, smooth=laplaceSmoothing)\n",
        "    mlp.fit(Xtrain, ytrain, validation_split=0.1, epochs=1000, batch_size=batchSize, callbacks=[EarlyStopping('val_auc', mode='max', patience=10, restore_best_weights=True)])\n",
        "\n",
        "    ypred_network = mlp(Xtest)\n",
        "    y_pred_network = np.argmax(np.array(ypred_network), axis=1) + 1\n",
        "    y_pred_KNN, _ = KNN_classifier(Xtrain_not_s, ytrain_not_s, Xtest_not_s, ytest, X_test)\n",
        "    y_pred_bag, _ = Bagging_n_extraTree(Xtrain_not_s, ytrain_not_s, Xtest_not_s, ytest, X_test)\n",
        "    y_pred_GB, _ = GradientBoosting(Xtrain_not_s, ytrain_not_s, Xtest_not_s, ytest, X_test)\n",
        "    y_pred_RF, _, y_pred_ETC, _ = RF_n_ExtraTree_classifier(Xtrain_not_s, ytrain_not_s, Xtest_not_s, ytest, X_test)\n",
        "    y_pred_XGB, _ = xgboost(Xtrain_not_s, ytrain_not_s, Xtest_not_s, ytest, X_test)\n",
        "    y_pred_GBM, _ = Light_gbm(Xtrain_not_s, ytrain_not_s, Xtest_not_s, ytest, X_test)\n",
        "    y_pred_vote, _ = Voting_RF_ETC_KNN(Xtrain_not_s, ytrain_not_s, Xtest_not_s, ytest, X_test)\n",
        "    y_pred_lr, _ = lr_classfier(Xtrain_not_s, ytrain_not_s, Xtest_not_s, ytest, X_test)\n",
        "    y_pred_ada, _ = ada_classifier(Xtrain_not_s, ytrain_not_s, Xtest_not_s, ytest, X_test)\n",
        "    y_pred_svc, _ = scv_classifier(Xtrain_not_s, ytrain_not_s, Xtest_not_s, ytest, X_test)\n",
        "\n",
        "    predictionList = [y_pred_network, y_pred_GB, y_pred_KNN, y_pred_bag, y_pred_RF, y_pred_ETC, y_pred_XGB, y_pred_GBM, y_pred_vote, y_pred_lr, y_pred_ada, y_pred_svc]\n",
        "    prediction = ensemble(predictionList)\n",
        "    score = f1_score(ytest.ravel(), prediction, average='weighted')\n",
        "\n",
        "    scoreList.append(score)\n",
        "    modelList.append(mlp)\n",
        "    clear_output()\n",
        "  print(f\"f1_score = {np.mean(scoreList)}\")\n",
        "  print(f\"f1_score list is {scoreList}\")\n",
        "  print(\"TEST FINISH\")\n",
        "elif TRAIN:\n",
        "  mlp = MLP_Model(X_s.shape[1], new_y.shape[1], encoder, hidden_layers, lr=learningRate, smooth=laplaceSmoothing)\n",
        "  mlp.fit(X_s, new_y, validation_split=0.1, epochs=1000, batch_size=batchSize, callbacks=[EarlyStopping('val_auc', mode='max', patience=10, restore_best_weights=True)])\n",
        "  mlp.save_weights(f'/content/drive/MyDrive/Again/mlp.hdf5')\n",
        "  mlp.trainable = False\n",
        "  print(\"TRAIN FINISH\")\n",
        "else:\n",
        "  mlp = MLP_Model(X_s.shape[1], new_y.shape[1], encoder, hidden_layers, lr=learningRate, smooth=laplaceSmoothing)\n",
        "  mlp.load_weights('/content/drive/MyDrive/Again/mlp.hdf5')\n",
        "  mlp.trainable = False\n",
        "  print(\"LOAD FINISH\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LOAD FINISH\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrYE_HAYmIpI"
      },
      "source": [
        "# **Validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9C9tKQJvmLpX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33f7c2df-8a94-4c83-d54d-e58d718b6289"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "TEST = False\n",
        "if not TEST:\n",
        "  y_val_pred = mlp(X_val_s, training=False)\n",
        "  y_pred_network = np.argmax(np.array(y_val_pred), axis=1) + 1\n",
        "\n",
        "  y_pred_KNN, _ = KNN_classifier(X, y, X_val, y_val, X_test)\n",
        "  y_pred_RF, _, y_pred_ETC, _ = RF_n_ExtraTree_classifier(X, y, X_val, y_val, X_test)\n",
        "  y_pred_XGB, _ = xgboost(X, y, X_val, y_val, X_test)\n",
        "  y_pred_GBM, _ = Light_gbm(X, y, X_val, y_val, X_test)\n",
        "  y_pred_GB, _ = GradientBoosting(X, y, X_val, y_val, X_test)\n",
        "  y_pred_bag, _ = Bagging_n_extraTree(X, y, X_val, y_val, X_test)\n",
        "  y_pred_vote, _ = Voting_RF_ETC_KNN(X, y, X_val, y_val, X_test)\n",
        "  y_pred_lr, _ = lr_classfier(X, y, X_val, y_val, X_test)\n",
        "  y_pred_ada, _ = ada_classifier(X, y, X_val, y_val, X_test)\n",
        "  y_pred_svc, _ = scv_classifier(X, y, X_val, y_val, X_test)\n",
        "  \n",
        "  predictionList = [y_pred_network, y_pred_GB, y_pred_KNN, y_pred_bag, y_pred_RF, y_pred_ETC, y_pred_XGB, y_pred_GBM, y_pred_vote, y_pred_lr, y_pred_ada, y_pred_svc]\n",
        "  prediction = ensemble(predictionList)\n",
        "  print()\n",
        "  print()\n",
        "  \n",
        "  f1_micro = f1_score(y_val.values.ravel(), prediction, average='micro')\n",
        "  print(\"f1_score (micro) =\", f1_micro)\n",
        "  f1_macro = f1_score(y_val.values.ravel(), prediction, average='macro')\n",
        "  print(\"f1_score (macro) =\", f1_macro)\n",
        "  f1_weighted = f1_score(y_val.values.ravel(), prediction, average='weighted')\n",
        "  print(\"f1_score (weighted) =\", f1_weighted)\n",
        "  print()\n",
        "  print(classification_report(y_val.values.ravel(), np.argmax(np.array(y_val_pred), axis=1) + 1))\n",
        "  print(confusion_matrix(y_val.values.ravel(), np.argmax(np.array(y_val_pred), axis=1) + 1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "f1 score --KNN: 0.9946066135727359\n",
            "\n",
            "f1_score_Randomforest:    0.9953174101825997\n",
            "\n",
            "f1_score___ExtraTress Classifier:    0.9956795986746309\n",
            "\n",
            "f1_score(weighted)___xgboost 0.9946039415911787\n",
            "f1_score(weighted)of === gbm:  0.9928051785721448\n",
            "f1_score(weighted)of === GradientBoosting:  0.989556974373595\n",
            "f1 score ---Bagging_eTree: 0.9949627252393135\n",
            "Hard Voting for f1 (weighted) socre:   0.9956795986746309\n",
            "f1_score is : 0.9888487024317972\n",
            "ada_classifier =  0.9928064248755569\n",
            "0.9827454602456348\n",
            "\n",
            "\n",
            "f1_score (micro) = 0.99568655643422\n",
            "f1_score (macro) = 0.9955418787972027\n",
            "f1_score (weighted) = 0.9956795986746309\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         1.0       1.00      0.98      0.99       512\n",
            "         2.0       0.98      0.99      0.99       552\n",
            "         3.0       1.00      0.97      0.99       327\n",
            "         4.0       0.99      1.00      0.99       408\n",
            "         5.0       0.98      1.00      0.99       622\n",
            "         6.0       1.00      0.99      0.99       361\n",
            "\n",
            "    accuracy                           0.99      2782\n",
            "   macro avg       0.99      0.99      0.99      2782\n",
            "weighted avg       0.99      0.99      0.99      2782\n",
            "\n",
            "[[504   6   0   1   1   0]\n",
            " [  1 549   0   0   2   0]\n",
            " [  0   3 318   0   6   0]\n",
            " [  1   1   0 406   0   0]\n",
            " [  0   0   0   1 620   1]\n",
            " [  0   0   0   3   1 357]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gTVrmX1Wmg3"
      },
      "source": [
        "# **Generate y_test.csv**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTBNhDnHbc6t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "361e85a1-1637-4c6a-cbd3-e7c8c1a881d5"
      },
      "source": [
        "\n",
        "y_pred_network = mlp(X_test_s, training=False)\n",
        "y_pred_network = np.argmax(np.array(y_pred_network), axis=1) + 1\n",
        "_, y_pred_KNN = KNN_classifier(X, y, X_val, y_val, X_test)\n",
        "_, y_pred_RF, _, y_pred_ETC = RF_n_ExtraTree_classifier(X, y, X_val, y_val, X_test)\n",
        "_, y_pred_XGB = xgboost(X, y, X_val, y_val, X_test)\n",
        "_, y_pred_GBM = Light_gbm(X, y, X_val, y_val, X_test)\n",
        "_, y_pred_GB = GradientBoosting(X, y, X_val, y_val, X_test)\n",
        "_, y_pred_bag = Bagging_n_extraTree(X, y, X_val, y_val, X_test)\n",
        "_, y_pred_vote = Voting_RF_ETC_KNN(X, y, X_val, y_val, X_test)\n",
        "_, y_pred_lr = lr_classfier(X, y, X_val, y_val, X_test)\n",
        "_, y_pred_ada = ada_classifier(X, y, X_val, y_val, X_test)\n",
        "_, y_pred_svc = scv_classifier(X, y, X_val, y_val, X_test)\n",
        "\n",
        "predictionList = [y_pred_network, y_pred_GB, y_pred_KNN, y_pred_bag, y_pred_RF, y_pred_ETC, y_pred_XGB, y_pred_GBM, y_pred_vote, y_pred_lr, y_pred_ada, y_pred_svc]\n",
        "prediction = ensemble(predictionList)\n",
        "\n",
        "prediction = pd.DataFrame(prediction)\n",
        "prediction.to_csv(\"/content/drive/MyDrive/Again/prediction.csv\", header=None, index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "f1 score --KNN: 0.9946066135727359\n",
            "\n",
            "f1_score_Randomforest:    0.9953174101825997\n",
            "\n",
            "f1_score___ExtraTress Classifier:    0.9956795986746309\n",
            "\n",
            "f1_score(weighted)___xgboost 0.9946039415911787\n",
            "f1_score(weighted)of === gbm:  0.9928051785721448\n",
            "f1_score(weighted)of === GradientBoosting:  0.9899169897371533\n",
            "f1 score ---Bagging_eTree: 0.9949627252393135\n",
            "Hard Voting for f1 (weighted) socre:   0.9956795986746309\n",
            "f1_score is : 0.9888487024317972\n",
            "ada_classifier =  0.9920881366655383\n",
            "0.9827454602456348\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlXr6qmNpZN9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpquoWzYpZP_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIOx1AsNpZSg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiDpQ7vspZW6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxIfBqW4pZZS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIcoO2gapZa3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}