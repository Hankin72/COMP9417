{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b633801",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import poisson\n",
    "\n",
    "import pylab\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "import numpy as np\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.decomposition import PCA, IncrementalPCA\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "import time\n",
    "import pylab\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA, IncrementalPCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.decomposition import PCA, IncrementalPCA\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "import time\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA, IncrementalPCA\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "from sklearn.decomposition import PCA, IncrementalPCA\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import datasets, linear_model, svm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0055b886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start : Wed Apr 28 00:01:42 2021\n",
      "\n",
      "Stop : Wed Apr 28 00:01:43 2021\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Start : %s\\n\" % time.ctime())\n",
    "\n",
    "X = pd.read_csv(\"./MLinTheUnknown-Data/X_train.csv\", header=None)\n",
    "y = pd.read_csv(\"./MLinTheUnknown-Data/y_train.csv\", header=None)\n",
    "\n",
    "X_val = pd.read_csv(\"./MLinTheUnknown-Data/X_val.csv\", header=None)\n",
    "y_val = pd.read_csv(\"./MLinTheUnknown-Data/y_val.csv\", header=None)\n",
    "\n",
    "X_test = pd.read_csv(\"./MLinTheUnknown-Data/X_test.csv\", header=None)\n",
    "\n",
    "print(\"Stop : %s\\n\" % time.ctime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e32a3590",
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2=[10,8,66,67,64,2,3,83,115,59,74,91,75,72,0,51,76,124,82,11,68,90,84,114,58,122,50,4,116,18,92,17,81,112,106]\n",
    "\n",
    "\n",
    "gini=[3,8,9,11,15,16,19,24,30,33,35,37,57,66,68,69,74,75,77,78,80,81,83,88,91,92,96,97,100,104,112,113,115,123,124]\n",
    "\n",
    "\n",
    "corr =[0,4,5,6,7,8,12,13,14,15,16,17,18,19,20,21,22,25,27,30,34,35,51,68,70,96,97,98,99,100,101,102]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34bb3721",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1\n",
    "def KNN(X, y, X_val, y_val, X_test, del_index):\n",
    "    \n",
    "    X_train = X[del_index]\n",
    "    X_val = X_val[del_index]\n",
    "\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    X_train = min_max_scaler.fit_transform(X_train)\n",
    "    X_val = min_max_scaler.transform(X_val)\n",
    "\n",
    "    y_train = np.array(y).ravel()\n",
    "    y_val = np.array(y_val).ravel()\n",
    "    \n",
    "    #KNN -classifier\n",
    "    clf_knn = KNeighborsClassifier(\n",
    "            n_neighbors = 1, \n",
    "            weights = 'uniform', \n",
    "            leaf_size =5,\n",
    "            algorithm = 'auto',\n",
    "            p=1)\n",
    "    \n",
    "    clf_knn.fit(X_train, y_train)\n",
    "    y_pred_knn= clf_knn.predict(X_val)\n",
    "    f1_knn= f1_score(y_val, y_pred_knn, average='weighted')\n",
    "    f1_knn =  round(f1_knn,5)\n",
    "    return f1_knn\n",
    "\n",
    "# 2\n",
    "def RF(X, y, X_val, y_val, X_test, del_index):\n",
    "    X_train = X[del_index]\n",
    "    X_val = X_val[del_index]\n",
    "\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    X_train = min_max_scaler.fit_transform(X_train)\n",
    "    X_val = min_max_scaler.transform(X_val)\n",
    "    \n",
    "    y_train = np.array(y).ravel()\n",
    "    y_val = np.array(y_val).ravel()\n",
    "\n",
    "    clf_rf = RandomForestClassifier(n_estimators = 160, \n",
    "                                    criterion='gini', \n",
    "                                    min_samples_leaf= 1, \n",
    "                                    max_features = 'auto',\n",
    "                                    min_samples_split =2,\n",
    "                                    random_state= 0\n",
    "                                   )\n",
    "    \n",
    "    clf_rf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred_rf = clf_rf.predict(X_val)\n",
    "    f1_rf= f1_score(y_val, y_pred_rf, average='weighted')\n",
    "    f1_rf= round(f1_rf,5)\n",
    "    return f1_rf\n",
    "\n",
    "# 3\n",
    "def extraTree(X, y, X_val, y_val, X_test, del_index):\n",
    "    \n",
    "    X_train = X[del_index]\n",
    "    X_val = X_val[del_index]\n",
    "\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    X_train = min_max_scaler.fit_transform(X_train)\n",
    "    X_val = min_max_scaler.transform(X_val)\n",
    "    \n",
    "    y_train = np.array(y).ravel()\n",
    "    y_val = np.array(y_val).ravel()\n",
    "      \n",
    "    clf_etc = ExtraTreesClassifier( criterion = 'gini',\n",
    "                                   max_features = 'auto',\n",
    "                                   random_state=0\n",
    "                                  )\n",
    "    clf_etc.fit(X_train, y_train)\n",
    "    y_pred_etc = clf_etc.predict(X_val)\n",
    "\n",
    "    f1_etc= f1_score(y_val, y_pred_etc, average='weighted')\n",
    "    f1_etc =round(f1_etc,5)\n",
    "    return f1_etc\n",
    "\n",
    "# 4\n",
    "def xgboost(X, y, X_val, y_val, X_test, del_index):\n",
    "    \n",
    "    X_train = X[del_index]\n",
    "    X_val = X_val[del_index]\n",
    "    \n",
    "    \n",
    "    X_train = np.array(X_train)\n",
    "    X_val = np.array(X_val)\n",
    "    y_train =np.array(y).ravel()\n",
    "    y_val =np.array(y_val).ravel()\n",
    "    # 归一化\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    X_train = min_max_scaler.fit_transform(X_train)\n",
    "    X_val = min_max_scaler.transform(X_val)\n",
    "\n",
    "    Validation = True\n",
    "    if Validation == True:\n",
    "        model = xgb.XGBClassifier(\n",
    "                max_depth=7,\n",
    "                learning_rate=0.05,\n",
    "                n_estimators=1000,\n",
    " \n",
    "                silent=True,\n",
    "                objective='multi:softmax',\n",
    "        )\n",
    "\n",
    "        model.fit(X_train,y_train)\n",
    "        y_pred_xgb = model.predict(X_val)\n",
    "        f1_xgb = f1_score(y_val, y_pred_xgb, average='weighted')\n",
    "        f1_xgb = round(f1_xgb, 5)\n",
    "        \n",
    "    return f1_xgb\n",
    "\n",
    "# 5\n",
    "def Light_gbm(X, y, X_val, y_val, X_test, del_index):\n",
    "    \n",
    "    X_train = X[del_index]\n",
    "    X_val = X_val[del_index]\n",
    "\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    X_train = min_max_scaler.fit_transform(X_train)\n",
    "    X_val = min_max_scaler.transform(X_val)\n",
    "\n",
    "    y_train = np.array(y).ravel()\n",
    "    y_val = np.array(y_val).ravel()\n",
    "    \n",
    "    gbm = LGBMClassifier(\n",
    "       num_leaves=31,\n",
    "        max_depth = -1, \n",
    "        learning_rate=0.1,\n",
    "        n_estimators= 120, \n",
    "        silent = True,\n",
    "        objective = 'multiclass' )\n",
    "    \n",
    "    gbm.fit(X_train,y_train)\n",
    "    y_pred_gbm = gbm.predict(X_val, num_iteration=46)\n",
    "    f1_gbm = f1_score(y_val, y_pred_gbm, average='weighted')\n",
    "    f1_gbm =round(f1_gbm,5)\n",
    "    \n",
    "    return f1_gbm\n",
    "\n",
    "# 6\n",
    "def GradientBoosting(X, y, X_val, y_val, X_test, del_index):\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    Xtrain = scaler.fit_transform(X)\n",
    "    Xval = scaler.transform(X_val)\n",
    "    \n",
    "    Xtrain =pd.DataFrame(Xtrain)\n",
    "    Xval = pd.DataFrame(Xval)\n",
    "    \n",
    "    Xtrain = Xtrain[del_index]\n",
    "    Xval = Xval[del_index]\n",
    "    \n",
    "    \n",
    "    \n",
    "    y= np.array(y).ravel()\n",
    "    y_val = np.array(y_val).ravel()\n",
    "    \n",
    "    \n",
    "    Xtrain = np.array(Xtrain)\n",
    "    Xval = np.array(Xval)\n",
    "\n",
    "    \n",
    "    \n",
    "    clf = GradientBoostingClassifier(n_estimators=150)\n",
    "    clf.fit(Xtrain, y)\n",
    "    \n",
    "    pred_y_val = clf.predict(Xval)\n",
    "\n",
    "    f1_gb = f1_score(y_val, pred_y_val, average='weighted')\n",
    "    f1_gb = round(f1_gb, 5)\n",
    "   \n",
    "    return f1_gb\n",
    "\n",
    "\n",
    "# 7\n",
    "def Bagging(X, y, X_val, y_val, X_test, del_index):\n",
    "    X_train = X[del_index]\n",
    "    X_val = X_val[del_index]\n",
    "    \n",
    "\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    X_train = min_max_scaler.fit_transform(X_train)\n",
    "    X_val = min_max_scaler.transform(X_val)\n",
    "\n",
    "    y_train = np.array(y).ravel()\n",
    "    y_val = np.array(y_val).ravel()\n",
    "    \n",
    "    bagging_model = BaggingClassifier(\n",
    "        base_estimator= ExtraTreeClassifier(criterion = 'entropy',max_features = 'auto',random_state=0),\n",
    "        n_estimators = 170,\n",
    "        max_samples = 6400,\n",
    "        random_state = 20\n",
    "    )  \n",
    "    bagging_model.fit(X_train, y_train )\n",
    "    y_pred_bag = bagging_model.predict(X_val)\n",
    "    \n",
    "    f1_bag= f1_score(y_val, y_pred_bag, average='weighted')\n",
    "    f1_bag = round(f1_bag,5)\n",
    "    \n",
    "    return f1_bag\n",
    "\n",
    "\n",
    "# 8\n",
    "def ada_classifier(X, y, X_val, y_val, X_test, del_index): # Adaboost\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X)\n",
    "    X_val = scaler.transform(X_val)\n",
    "    \n",
    "\n",
    "    X_train = pd.DataFrame(X_train)\n",
    "    X_val = pd.DataFrame(X_val)\n",
    "    \n",
    "    \n",
    "    X_train = X_train[del_index]\n",
    "    X_val = X_val[del_index]\n",
    "    \n",
    "    \n",
    "    X_train = np.array(X_train)\n",
    "    X_val =np.array(X_val)\n",
    "    y_train = np.array(y).ravel()\n",
    "    y_val = np.array(y_val).ravel()\n",
    "    \n",
    "    clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=12), n_estimators=150)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred_val = clf.predict(X_val)\n",
    "    \n",
    "    f1_ada = f1_score(y_val, y_pred_val, average='weighted')\n",
    "    f1_ada = round(f1_ada,5)\n",
    "    \n",
    "    return f1_ada\n",
    "  \n",
    "# 9\n",
    "def scv_classifier(X, y, X_val, y_val, X_test, del_index):   # SVM method\n",
    "    # normalization\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X) \n",
    "    X_val = scaler.transform(X_val)\n",
    "    \n",
    "    X_train = pd.DataFrame(X_train)\n",
    "    X_val = pd.DataFrame(X_val)\n",
    "    \n",
    "    \n",
    "    X_train = X_train[del_index]\n",
    "    X_val = X_val[del_index]\n",
    "    \n",
    "    \n",
    "    X_train = np.array(X_train)\n",
    "    X_val =np.array(X_val)\n",
    "    y_train = np.array(y).ravel()\n",
    "    y_val = np.array(y_val).ravel()\n",
    "    \n",
    "    \n",
    "    # train\n",
    "    svc = svm.SVC(C=2.0)\n",
    "    svc.fit(X_train, y_train)\n",
    "\n",
    "    y_pred_val = svc.predict(X_val)\n",
    "    f1_svm = f1_score(y_val, y_pred_val, average='weighted')\n",
    "    f1_svm = round(f1_svm,5)\n",
    "    return f1_svm\n",
    "\n",
    "# 10\n",
    "def lr_classfier(X, y, X_val, y_val, X_test, del_index): \n",
    "    scaler = StandardScaler()\n",
    "    Xtrain = scaler.fit_transform(X)\n",
    "    X_val = scaler.transform(X_val)\n",
    "    \n",
    "    \n",
    "    Xtrain =pd.DataFrame(Xtrain)\n",
    "    Xval = pd.DataFrame(X_val)\n",
    "    \n",
    "    Xtrain = Xtrain[del_index]\n",
    "    Xval = Xval[del_index]\n",
    "    \n",
    "    \n",
    "    \n",
    "    y= np.array(y).ravel()\n",
    "    y_val = np.array(y_val).ravel()\n",
    "    \n",
    "    \n",
    "    Xtrain = np.array(Xtrain)\n",
    "    Xval = np.array(Xval)\n",
    "   \n",
    "   \n",
    "    \n",
    "\n",
    "    clf = LogisticRegression(C=10.1,solver = 'newton-cg', max_iter=1000)\n",
    "\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    pred_val_y = clf.predict(X_val)\n",
    "    \n",
    "    f1_lr = f1_score(y_val, pred_val_y, average='weighted')\n",
    "    f1_lr =  round(f1_lr,5)\n",
    "    \n",
    "    return f1_lr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12e77f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#  for chi2\n",
    "chi2_knn = []\n",
    "chi2_randomForest =[]\n",
    "\n",
    "chi2_extratree =[]\n",
    "chi2_xgboost=[]\n",
    "\n",
    "chi2_light_gbm =[]\n",
    "chi2_gradientBoosting = []\n",
    "\n",
    "chi2_bagging= []\n",
    "chi2_adaboost = []\n",
    "\n",
    "chi2_svm = []\n",
    "chi2_logistic_regression = []\n",
    "\n",
    "\n",
    "# 1\n",
    "for i in range(1, len(chi2)):\n",
    "    chi2_temp = chi2[:i]\n",
    "    \n",
    "    f1 = KNN(X, y, X_val, y_val, X_test, chi2_temp)\n",
    "    chi2_knn.append(f1)\n",
    "print(\"========================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e2498a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n"
     ]
    }
   ],
   "source": [
    "# 2\n",
    "for i in range(1, len(chi2)):\n",
    "    chi2_temp = chi2[:i]\n",
    "    f2 = RF(X, y, X_val, y_val, X_test, chi2_temp)\n",
    "    \n",
    "    chi2_randomForest.append(f2) \n",
    "    \n",
    "print(\"====================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99aba7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3\n",
    "for i in range(1, len(chi2)):\n",
    "    chi2_temp = chi2[:i]\n",
    "    f3 = extraTree(X, y, X_val, y_val, X_test, chi2_temp)\n",
    "    chi2_extratree.append(f3) \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c5a39b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-11c336b4ee5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchi2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mchi2_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchi2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mf4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgboost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchi2_temp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mchi2_xgboost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-26219d47a040>\u001b[0m in \u001b[0;36mxgboost\u001b[0;34m(X, y, X_val, y_val, X_test, del_index)\u001b[0m\n\u001b[1;32m    104\u001b[0m         )\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0my_pred_xgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mf1_xgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_xgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'weighted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/COMP9444/lib/python3.7/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[1;32m    730\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"objective\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/COMP9444/lib/python3.7/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[1;32m    214\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/COMP9444/lib/python3.7/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/COMP9444/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[0;32m-> 1109\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m   1110\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 4\n",
    "for i in range(1, len(chi2)):\n",
    "    chi2_temp = chi2[:i]\n",
    "    f4 = xgboost(X, y, X_val, y_val, X_test, chi2_temp)\n",
    "    chi2_xgboost.append(f4) \n",
    "    \n",
    "print(\"====================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e182283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5\n",
    "for i in range(1, len(chi2)):\n",
    "    chi2_temp = chi2[:i]\n",
    "    f5 = Light_gbm(X, y, X_val, y_val, X_test, chi2_temp)\n",
    "    chi2_light_gbm.append(f5) \n",
    "    \n",
    "print(\"====================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e39de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6\n",
    "for i in range(1, len(chi2)):\n",
    "    chi2_temp = chi2[:i]\n",
    "    f6 = GradientBoosting(X, y, X_val, y_val, X_test, chi2_temp)\n",
    "    chi2_gradientBoosting.append(f6) \n",
    "    \n",
    "print(\"====================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef14b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7\n",
    "for i in range(1, len(chi2)):\n",
    "    chi2_temp = chi2[:i]\n",
    "    f7 = Bagging(X, y, X_val, y_val, X_test, chi2_temp)\n",
    "    chi2_bagging.append(f7) \n",
    "    \n",
    "print(\"====================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7885aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8\n",
    "for i in range(1, len(chi2)):\n",
    "    chi2_temp = chi2[:i]\n",
    "    f8 = ada_classifier(X, y, X_val, y_val, X_test, chi2_temp)\n",
    "    chi2_adaboost.append(f8) \n",
    "    \n",
    "print(\"====================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a1611c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9\n",
    "for i in range(1, len(chi2)):\n",
    "    chi2_temp = chi2[:i]\n",
    "    f9 = scv_classifier(X, y, X_val, y_val, X_test, chi2_temp)\n",
    "    chi2_svm.append(f9) \n",
    "    \n",
    "print(\"====================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5a0238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10\n",
    "for i in range(1, len(chi2)):\n",
    "    chi2_temp = chi2[:i]\n",
    "    f10 = lr_classfier(X, y, X_val, y_val, X_test, chi2_temp)\n",
    "    chi2_logistic_regression.append(f10) \n",
    "    \n",
    "print(\"====================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d640dc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  for chi2\n",
    "# chi2_knn = []\n",
    "# chi2_randomForest =[]\n",
    "\n",
    "# chi2_extratree =[]\n",
    "# chi2_xgboost=[]\n",
    "\n",
    "# chi2_light_gbm =[]\n",
    "# chi2_gradientBoosting = []\n",
    "\n",
    "# chi2_bagging= []\n",
    "# chi2_adaboost = []\n",
    "\n",
    "# chi2_svm = []\n",
    "# chi2_logistic_regression = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x = [i for i in range(1,len(chi2))]\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.title(\"Chi2-Dims on different models\")\n",
    "# 1\n",
    "plt.plot(x, chi2_knn, color='blue', label=\"KNN\")\n",
    "# 2\n",
    "plt.plot(x, chi2_randomForest, color='Orange', label=\"RandomForest\")\n",
    "# 3\n",
    "plt.plot(x, chi2_extratree, color='Lime', label=\"ExtraTree\")\n",
    "# 4\n",
    "plt.plot(x, chi2_xgboost, color='Red', label=\"Xgboost\")\n",
    "# 5\n",
    "plt.plot(x, chi2_light_gbm, color='DarkRed', label=\"Light-gbm\")\n",
    "# 6\n",
    "plt.plot(x, chi2_gradientBoosting, color='Chocolate', label=\"GradientBoost\")\n",
    "# 7\n",
    "plt.plot(x, chi2_bagging, color='lightgreen', label=\"Bagging\")\n",
    "# 8\n",
    "plt.plot(x, chi2_adaboost, color='black', label=\"Adaboost\")\n",
    "# 9\n",
    "plt.plot(x, chi2_svm, color='Orchid', label=\"SVM\")\n",
    "# 10\n",
    "plt.plot(x, chi2_logistic_regression, color='lightblue', label=\"LogisticRegression\")\n",
    "# ##############\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.4,linestyle=':')\n",
    "\n",
    "plt.xlabel(\"Num of Features\")\n",
    "# plt.xticks(x, random_sizes, rotation=0)\n",
    "plt.ylabel(\"f1-score(weighted)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f13aa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097004db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
